---
title: Analytics Report Shell
author: 
  - name: Zachary Vandecar
    email: zachary.vandecar@snhu
    affiliations: 
      - name: Southern New Hampshire University
format: html
date: 9/15/2024
date-modified: today
date-format: long
theme: flatly
toc: true
code-fold: true
---

# Quarto





these work within individual chunks or in the header
"#| message: false" #does not display output to code in report
"#| echo: false"  #does not display code in report, but does run
"#| code-fold: true" #makes the code a dropdown



```{r}
#| message: false



library(tidyverse)
library(tidymodels)
library(dplyr)
library(skimr)
library(ggplot2)
library(patchwork)

hits <- read.csv("https://raw.githubusercontent.com/agmath/agmath.github.io/refs/heads/master/data/classification/battedballs.csv")

parks <- read.csv("https://raw.githubusercontent.com/agmath/agmath.github.io/refs/heads/master/data/classification/park_dimensions.csv")





```


## joining data
```{r}

joined_hits <- hits %>% left_join(parks, by = c("park"="park") )


```
we joined `hits` and `parks` data together for a dataset with `r nrow(joined_hits)` rows and `r ncol(joined_hits)` columns


## splitting into training and testing data


```{r}


set.seed(434)
 
#marks the data basically
data_splits <- initial_split(joined_hits, 0.85, strata = is_home_run ) #USE STRATA TO SPLIT A VARIABLES VALUES PROPORTIONALLY
#can put list in for multiple strata. 
#more stratified, less random, losing some potential patterns 
#that line tries to make sure that is_home_run is split proportionally 


#these are actually making datasets out of the markings
test <- testing(data_splits) 
train <- training(data_splits)





```


## exploratory data analysis


### data poking (head, glimpse, skim)
```{r}

hits |> head(10)

parks %>% head(3)

joined_hits %>% head()

joined_hits %>%  glimpse()

joined_hits %>% skim()


```


### filter and select

```{r}

train %>% filter(is_home_run == 1)

train %>% filter(launch_angle > 45 & is_home_run)

train %>% filter(launch_angle > 45 & is_home_run) %>% select(launch_angle, launch_speed)


```

### building new variables from old ones....."Feature Engineering"

```{r}

train %>%
  mutate(fast_pitch = ifelse(pitch_mph > 100, "yes", "no") ) #flagging a row as true if the pitch_speed is greater than 100
#so, to be clear, it adds a new column that is either yes or no 


#can use step mutate when making a model 


```
### getting summary statistics

```{r}

train %>% 
  count(is_home_run) %>% 
  mutate(prop = n /sum(n))



```


### summaries
```{r}

train %>% 
  filter(!is.na(launch_angle))  %>%         #! -> negation....not
  summarize(pct_hr = mean(is_home_run),
            min_angle = min(launch_angle),
            mean_angle = mean(launch_angle),
            median_angle = median(launch_angle),
            max_angle = max(launch_angle),
            sd_angle = sd(launch_angle)
  )
  



```

#### group summaries

```{r}


train %>% 
  group_by(is_home_run) %>% 
  filter(!is.na(launch_angle))  %>%         #! -> negation....not
  summarize(pct_hr = mean(is_home_run),
            min_angle = min(launch_angle),
            mean_angle = mean(launch_angle),
            median_angle = median(launch_angle),
            max_angle = max(launch_angle),
            sd_angle = sd(launch_angle)
  )

```
### ***Data Visualization***

```{r}

#themes can be found online....this one is black and white and adjusts font sizes automatically

theme_set(theme_bw(base_size = 14))




```





Plot the distribution of home-runs versus non-home-runs (is_home_run)
```{r}

train %>%
  ggplot() + 
  geom_bar(aes(x=is_home_run)) + 
  labs(x="is_home_run")





```



Plot the distribution of pitch_mph

```{r}

train %>% 
  ggplot() +
  geom_histogram(aes(x=pitch_mph,y = after_stat(density)), bins = 40, ) + 
  geom_density(aes(x = pitch_mph))



`````````


Plot the distribution of launch_speed

```{r}

```

Plot the distribution of launch_angle

```{r}

```


plot number of homeruns per stadium 

```{r}


#doesnt ever need y, because it is counting how many of each class given there are
train %>% 
  filter(is_home_run == 1) %>% 
  ggplot() +
  geom_bar(aes(x=NAME))



```
recode logicals (booleans) to character (mutate with as.character)

classification outcome needs to be a factor (categorical) (mutate with as.factor() or factor() )




## Model COnstruction, Evaluation, and Tuning

prep our data:

```{r}

#tidy models for classification doesnt want numbers as result...so changing 1/0 to character
hits_prepped <- joined_hits %>% 
  mutate(is_home_run = ifelse(is_home_run ==1, "Yes", "No")) %>% 
  mutate(is_home_run = factor(is_home_run, levels = c("No", "Yes"))) 
#creates a factor out of it....connects all the "Yes" rows...they are same group not just characters
 
# the levels argument lets you re-order the categories/levels. You can basically make one of them more important...for binary you can make the important rare one represented by a 1 rather than a 0 (true rather than false)....but not logicals lol


set.seed(434)
data_splits <- initial_split(hits_prepped, 0.85, strata = is_home_run)
#I think strata splits is_home_run equally in the split...or at least ensures its in both


train <- training(data_splits)
test <- testing(data_splits)






#are decision trees binary?


dt_spec <- decision_tree() %>%
  set_engine("rpart") %>% 
  set_mode("classification")


dt_rec <- recipe(is_home_run ~ launch_speed + launch_angle + pitch_mph + pitch_name + Cover, data = train) %>% 
  step_impute_median(all_numeric_predictors()) %>% #Fill in missing numeric values with median of that feature  
  step_impute_mode(all_nominal_predictors()) %>%  #using mode to fill in missing categorical values 
  step_dummy(all_nominal_predictors()) #nominal is categorical


#baked is in the recipe right? prepped dataset? can we see it? mess with it?

dt_wf <- workflow() %>% 
  add_model(dt_spec) %>% 
  add_recipe(dt_rec)


dt_fit <- dt_wf %>% fit(train)



```

looking at fit model: 

```{r}


dt_fit %>% 
  extract_fit_engine() %>% 
  rpart.plot::rpart.plot() #show decision tree
  
```


assess model performance 

```{r}



dt_fit %>%
  augment(train) %>% 
  accuracy(is_home_run, .pred_class) #scoring function...comparing true outcome to predicted

#this answer is bloated, and probably overfit

```


using test data to assess:
```{r}


dt_fit %>% 
  augment(test) %>% 
  accuracy(is_home_run, .pred_class)

```
if test estimate is worse or way worse, we probably overfit. If the performance increases, it's possible we are underfitting. No new work should be done after test is used...defeats the purpose, you are overfitting.


We still want unbiased accuracy metrics before we use test (and finish our process). So we use cross-validation. 

```{r}

train_folds <- vfold_cv(train, v = 10, strata = is_home_run) #yea here too, ensuring is_home_run is split equally/proportionally

dt_cv_results <-dt_wf %>% 
  fit_resamples(train_folds)


dt_cv_results %>% 
  collect_metrics()








```


tuning our model

```{r}

dt_spec <- decision_tree(tree_depth = tune()) %>% 
  set_engine("rpart") %>% 
  set_mode("classification")

dt_wf <- workflow() %>% 
  add_model(dt_spec) %>% 
  add_recipe(dt_rec)


dt_tune_results <- dt_wf %>% 
  tune_grid(
    resamples = train_folds,
    grid = 10 #10 different tree depths
  )

dt_tune_results %>% 
  show_best(n=10, METRIC = "accuracy")




```

